Apache Spark大数据分析入门（一）
	全文共包括四个部分：
		第一部分：Spark入门，介绍如何使用Shell及RDDs
		第二部分：介绍Spark SQL、Dataframes及如何结合Spark与Cassandra一起使用
		第三部分：介绍Spark MLlib和Spark Streaming
		第四部分：介绍Spark Graphx图计算
	
	整个生态系统构建在Spark内核引擎之上，内核使得Spark具备快速的内存计算能力，也使得其API支持Java、Scala,、Python、R四种编程语言。Streaming具备实时流数据的处理能力。Spark SQL使得用户使用他们最擅长的语言查询结构化数据，DataFrame位于Spark SQL的核心，DataFrame将数据保存为行的集合，对应行中的各列都被命名，通过使用DataFrame，可以非常方便地查询、绘制和过滤数据。MLlib为Spark中的机器学习框架。Graphx为图计算框架，提供结构化数据的图计算能力。
	下面总结一下Spark从开始到结果的运行过程：

	创建某种数据类型的RDD
	对RDD中的数据进行转换操作，例如过滤操作
	在需要重用的情况下，对转换后或过滤后的RDD进行缓存
	在RDD上进行action操作，例如提取数据、计数、存储数据到Cassandra等。
	下面给出的是RDD的部分转换操作清单：

	filter()
	map()
	sample()
	union()
	groupbykey()
	sortbykey()
	combineByKey()
	subtractByKey()
	mapValues()
	Keys()
	Values()
	下面给出的是RDD的部分action操作清单：

	collect()
	count()
	first()
	countbykey()
	saveAsTextFile()
	reduce()
	take(n)
	countBykey()
	collectAsMap()
	lookup(key)